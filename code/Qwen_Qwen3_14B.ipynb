{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6D6lZeoLzBVM"
      },
      "outputs": [],
      "source": [
        "!pip install -q \"transformers>=4.51.0\" accelerate bitsandbytes sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# Qwen3-8B\n",
        "model_name = \"Qwen/Qwen3-14B\"\n",
        "\n",
        "# ▼ 4bit 量子化の設定\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,                         # ★ 4bit 量子化\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,           # さらに省メモリ\n",
        "    bnb_4bit_quant_type=\"nf4\",                # よく使われる方式\n",
        ")\n",
        "\n",
        "# tokenizer & model をロード\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",            # GPU があれば自動で使う\n",
        "    quantization_config=bnb_config,\n",
        ")\n"
      ],
      "metadata": {
        "id": "kZv586p_zHVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def summarize_qwen3(text: str, target_chars: int = 120) -> str:\n",
        "    \"\"\"\n",
        "    Qwen3 を使って日本語要約する（英語禁止、思考モードOFF）。\n",
        "    \"\"\"\n",
        "\n",
        "    user_prompt = (\n",
        "        f\"次の文章を、日本語で、だいたい {target_chars} 文字程度に自然に要約してください。\\n\"\n",
        "        f\"・出力は必ず日本語のみ\\n\"\n",
        "        f\"・英語や推論過程（<think>など）を出力しない\\n\"\n",
        "        f\"・結果だけ簡潔に\\n\\n\"\n",
        "        f\"【文章】\\n{text}\\n\\n【要約】\"\n",
        "    )\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": (\n",
        "                \"あなたは日本語専用の要約アシスタントです。\"\n",
        "                \"英語や他の言語、推論過程・自己コメントを絶対に出力してはいけません。\"\n",
        "                \"回答は【要約】の後の文章のみとします。\"\n",
        "            ),\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ]\n",
        "\n",
        "    chat_text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "\n",
        "        assistant_response=False,\n",
        "        enable_thinking=False,\n",
        "        response_format=\"text\",\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(chat_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.3,  # 少し下げて暴れにくく\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    gen_ids = generated[0][inputs[\"input_ids\"].shape[1]:]\n",
        "    output = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
        "\n",
        "    # 余計な部分があったらカット\n",
        "    if \"【要約】\" in output:\n",
        "        output = output.split(\"【要約】\", 1)[-1].strip()\n",
        "\n",
        "    # <think> や英語が誤って出ても除去\n",
        "    output = re.sub(r\"<think>.*?</think>\", \"\", output, flags=re.DOTALL).strip()\n",
        "    output = re.sub(r\"[a-zA-Z]+\", \"\", output).strip()\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "-Hq02V8UzImB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = input(\"要約したい文章を貼ってください：\\n\")\n",
        "target = int(input(\"だいたい何文字くらいで要約してほしいですか？：\"))\n",
        "\n",
        "print(\"\\n--- Qwen3-8B 要約結果 ---\\n\")\n",
        "summary = summarize_qwen3(text, target_chars=target)\n",
        "print(summary)\n",
        "print(\"\\n【実際の文字数】：\", len(summary))\n"
      ],
      "metadata": {
        "id": "nLf9GsTvzKwX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}