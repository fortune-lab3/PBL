{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aay6GgfqW7bZ"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# âœ… è¦ç´„ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œç‰ˆï¼‰\n",
        "# ==============================\n",
        "import os\n",
        "from transformers import pipeline\n",
        "from huggingface_hub import login\n",
        "login(token=\"ã“ã“ã«ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥ã‚Œã‚‹\")\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«IDï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼‰\n",
        "MODEL_ID = \"google/gemma-2-2b-it\"\n",
        "\n",
        "def summarize_local(text: str, max_tokens: int = 280, temperature: float = 0.3, char_limit: int = 100):\n",
        "    \"\"\"\n",
        "    Gemma-2-2b-it ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã§ä½¿ã£ã¦æ—¥æœ¬èªã®è¦ç´„ã‚’ç”Ÿæˆã—ã¾ã™ã€‚\n",
        "    char_limit: è¦ç´„ã®æ–‡å­—æ•°ä¸Šé™\n",
        "    \"\"\"\n",
        "    print(\"ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...ï¼ˆåˆå›ã¯æ•°åˆ†ã‹ã‹ã‚Šã¾ã™ï¼‰\")\n",
        "\n",
        "    generator = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=MODEL_ID,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=\"auto\"\n",
        "    )\n",
        "\n",
        "    prompt = (\n",
        "        f\"ã‚ãªãŸã¯æ—¥æœ¬èªã®è¦ç´„ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®æ–‡ç« ã‚’{char_limit}æ–‡å­—ã§è‡ªç„¶ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚\\n\"\n",
        "        \"ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ãªã©ã®ä¸è¦ãªæƒ…å ±ã¯çœãã€èª­ã¿ã‚„ã™ãã¾ã¨ã‚ã¦ãã ã•ã„ã€‚\\n\"\n",
        "        \"è¦ç´„ã¯1ã¤ã ã‘ç”Ÿæˆã—ã€ç¹°ã‚Šè¿”ã•ãªã„ã§ãã ã•ã„ã€‚\\n\\n\"\n",
        "        f\"ã€å…¥åŠ›ã€‘\\n{text}\\n\\nã€è¦ç´„ã€‘\"\n",
        "    )\n",
        "\n",
        "    result = generator(\n",
        "        prompt,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=generator.tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    generated_text = result[0]['generated_text']\n",
        "    summary = generated_text.replace(prompt, \"\").strip()\n",
        "\n",
        "    # è¡Œã®é‡è¤‡å‰Šé™¤\n",
        "    lines = summary.split('\\n')\n",
        "    unique_lines = []\n",
        "    for line in lines:\n",
        "        if line.strip() and line not in unique_lines:\n",
        "            unique_lines.append(line.strip())\n",
        "    summary = '\\n'.join(unique_lines[:3])\n",
        "\n",
        "    # æ–‡å­—æ•°åˆ¶é™ï¼ˆã‚ªãƒ¼ãƒãƒ¼ã—ãŸã‚‰åˆ‡ã‚‹ï¼‰\n",
        "    if len(summary) > char_limit:\n",
        "        summary = summary[:char_limit] + \"...\"\n",
        "\n",
        "    return summary, len(summary)  # â† â˜… æ–‡å­—æ•°ã‚‚è¿”ã™ï¼\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# âœ… å…¥åŠ› & å®Ÿè¡Œ\n",
        "# ==============================\n",
        "print(\"ğŸ“ è¦ç´„ã—ãŸã„ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆè¤‡æ•°è¡ŒOKï¼‰\\n\")\n",
        "text = \"\"\n",
        "while True:\n",
        "    line = input()\n",
        "    if line.strip() == \"\":\n",
        "        break\n",
        "    text += line + \"\\n\"\n",
        "\n",
        "# æ–‡å­—æ•°æŒ‡å®š\n",
        "try:\n",
        "    char_limit = int(input(\"è¦ç´„ã®æ–‡å­—æ•°ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ï¼ˆä¾‹: 100ï¼‰: \"))\n",
        "except ValueError:\n",
        "    char_limit = 100\n",
        "    print(\"ç„¡åŠ¹ãªå…¥åŠ›ãªã®ã§ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®100æ–‡å­—ã«è¨­å®šã—ã¾ã™ã€‚\")\n",
        "\n",
        "print(\"\\n--- è¦ç´„çµæœ ---\\n\")\n",
        "summary, length = summarize_local(text, char_limit=char_limit)\n",
        "\n",
        "print(summary)\n",
        "print(f\"\\nğŸ“ è¦ç´„ã®æ–‡å­—æ•°: {length} æ–‡å­—\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fr-cuqvxXhYp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
