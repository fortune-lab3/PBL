{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xuls4_2bbGQb"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers sentencepiece python-docx pandasimport os\n",
        "import glob\n",
        "import pandas as pd\n",
        "from docx import Document\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "import time\n",
        "from google.colab import drive # Driveãƒã‚¦ãƒ³ãƒˆã®ãŸã‚ã«è¿½åŠ \n",
        "\n",
        "# --- ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®æº–å‚™ (æµç”¨) ---\n",
        "model_name = \"tsmatz/mt5_summarize_japanese\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# GPUãŒåˆ©ç”¨å¯èƒ½ã§ã‚ã‚Œã°ä½¿ç”¨\n",
        "if torch.cuda.is_available():\n",
        "    model.to('cuda')\n",
        "\n",
        "def summarize(text: str, target_chars=120):\n",
        "    \"\"\"æŒ‡å®šã•ã‚ŒãŸæ–‡å­—æ•°ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦ç´„ã™ã‚‹é–¢æ•°\"\"\"\n",
        "    max_length = int(target_chars * 1.3)\n",
        "\n",
        "    prompt = (\n",
        "        f\"æ¬¡ã®æ–‡ç« ã‚’ã€ã ã„ãŸã„ {target_chars} æ–‡å­—ãã‚‰ã„ã®é•·ã•ã§è‡ªç„¶ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚\\n\"\n",
        "        f\"ã€æ–‡ç« ã€‘{text}\\n\"\n",
        "        \"ã€è¦ç´„ã€‘\"\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = inputs.to('cuda')\n",
        "\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_length=max_length,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "def read_docx(file_path):\n",
        "    \"\"\"Wordãƒ•ã‚¡ã‚¤ãƒ« (.docx) ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°\"\"\"\n",
        "    try:\n",
        "        document = Document(file_path)\n",
        "        full_text = \"\\n\".join([p.text.strip() for p in document.paragraphs if p.text.strip()])\n",
        "        return full_text\n",
        "    except Exception as e:\n",
        "        print(f\"  ğŸš¨ Wordãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def summarize_folder_to_csv(folder_path: str, target_chars: int, output_csv: str):\n",
        "    \"\"\"ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ã™ã¹ã¦ã®.docxãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¦ç´„ã—ã€çµæœã‚’CSVã«ä¿å­˜ã™ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°\"\"\"\n",
        "\n",
        "    # ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
        "    if not os.path.isdir(folder_path):\n",
        "        print(f\"ğŸ›‘ ã‚¨ãƒ©ãƒ¼: æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„: {folder_path}\")\n",
        "        return\n",
        "\n",
        "    print(f\"--- ãƒ•ã‚©ãƒ«ãƒ€å†…ã®.docxãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ä¸­: {folder_path} ---\")\n",
        "\n",
        "    file_list = glob.glob(os.path.join(folder_path, \"*.docx\"))\n",
        "\n",
        "    if not file_list:\n",
        "        print(\"ğŸ›‘ æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€å†…ã«.docxãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
        "        return\n",
        "\n",
        "    results = []\n",
        "    print(f\"--- âœ… {len(file_list)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¦‹ã¤ã‘ã¾ã—ãŸã€‚å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ ---\")\n",
        "\n",
        "    for i, file_path in enumerate(file_list):\n",
        "        start_time = time.time()\n",
        "        file_name = os.path.basename(file_path)\n",
        "        print(f\"\\n[{i+1}/{len(file_list)}] ğŸ“„ å‡¦ç†ä¸­: {file_name}\")\n",
        "\n",
        "        original_text = read_docx(file_path)\n",
        "\n",
        "        if original_text and original_text.strip():\n",
        "            try:\n",
        "                summary_text = summarize(original_text, target_chars=target_chars)\n",
        "                actual_length = len(summary_text)\n",
        "                elapsed_time = time.time() - start_time\n",
        "                print(f\"   -> è¦ç´„å®Œäº† (æ–‡å­—æ•°: {actual_length}, å‡¦ç†æ™‚é–“: {elapsed_time:.2f}ç§’)\")\n",
        "\n",
        "                results.append({\n",
        "                    \"ãƒ•ã‚¡ã‚¤ãƒ«å\": file_name,\n",
        "                    \"å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆå…ˆé ­100æ–‡å­—ï¼‰\": original_text[:100].replace('\\n', ' ') + \"...\",\n",
        "                    \"è¦ç´„æ–‡å­—æ•°_å¸Œæœ›\": target_chars,\n",
        "                    \"è¦ç´„æ–‡å­—æ•°_å®Ÿéš›\": actual_length,\n",
        "                    \"è¦ç´„çµæœ\": summary_text\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"   ğŸš¨ è¦ç´„ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")\n",
        "                results.append({\n",
        "                    \"ãƒ•ã‚¡ã‚¤ãƒ«å\": file_name,\n",
        "                    \"å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆå…ˆé ­100æ–‡å­—ï¼‰\": \"å‡¦ç†ã‚¨ãƒ©ãƒ¼\",\n",
        "                    \"è¦ç´„æ–‡å­—æ•°_å¸Œæœ›\": target_chars,\n",
        "                    \"è¦ç´„æ–‡å­—æ•°_å®Ÿéš›\": 0,\n",
        "                    \"è¦ç´„çµæœ\": f\"è¦ç´„ã‚¨ãƒ©ãƒ¼: {e}\"\n",
        "                })\n",
        "        else:\n",
        "            print(\"   âš ï¸ ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã‹ã€èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚\")\n",
        "            results.append({\n",
        "                \"ãƒ•ã‚¡ã‚¤ãƒ«å\": file_name,\n",
        "                \"å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆå…ˆé ­100æ–‡å­—ï¼‰\": \"ãƒ†ã‚­ã‚¹ãƒˆãªã—/èª­ã¿è¾¼ã¿å¤±æ•—\",\n",
        "                \"è¦ç´„æ–‡å­—æ•°_å¸Œæœ›\": target_chars,\n",
        "                \"è¦ç´„æ–‡å­—æ•°_å®Ÿéš›\": 0,\n",
        "                \"è¦ç´„çµæœ\": \"å‡¦ç†ã‚¹ã‚­ãƒƒãƒ—\"\n",
        "            })\n",
        "\n",
        "    # çµæœã‚’Pandas DataFrameã«ã—ã¦CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜\n",
        "    if results:\n",
        "        df = pd.DataFrame(results)\n",
        "        df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
        "        print(f\"\\n--- âœ… ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚çµæœã‚’CSVã«ä¿å­˜ã—ã¾ã—ãŸ: {output_csv} ---\")\n",
        "    else:\n",
        "        print(\"\\n--- å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„ãŸã‚ã€CSVãƒ•ã‚¡ã‚¤ãƒ«ã¯ä½œæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚ ---\")\n",
        "\n",
        "\n",
        "# ==================================\n",
        "# === å®Ÿè¡Œéƒ¨åˆ† (Google Colab / ãƒ‘ã‚¹å›ºå®š) ===\n",
        "# ==================================\n",
        "\n",
        "print(\"--- ğŸ“‚ Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆã—ã¾ã™ ---\")\n",
        "# èªè¨¼ãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿè¡Œã—ã€Google Driveã‚’ '/content/drive' ã«ãƒã‚¦ãƒ³ãƒˆ\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŒ‡å®šã«åŸºã¥ãã€ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã‚’å›ºå®š\n",
        "FOLDER_NAME = \"tv\"\n",
        "folder_path = f\"/content/drive/MyDrive/{FOLDER_NAME}\"\n",
        "\n",
        "print(f\"\\n--- ğŸ¯ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ•ã‚©ãƒ«ãƒ€: {folder_path} ---\")\n",
        "\n",
        "# ç›®æ¨™æ–‡å­—æ•°ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å…¥åŠ›ã—ã¦ã‚‚ã‚‰ã†\n",
        "try:\n",
        "    target = int(input(\"ã ã„ãŸã„ä½•æ–‡å­—ãã‚‰ã„ã§è¦ç´„ã—ã¦ã»ã—ã„ã§ã™ã‹ï¼Ÿï¼š\"))\n",
        "except ValueError:\n",
        "    print(\"å…¥åŠ›ãŒä¸æ­£ã§ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®120æ–‡å­—ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\")\n",
        "    target = 120\n",
        "\n",
        "# å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¨­å®š\n",
        "# çµæœã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€èª­ã¿è¾¼ã¿å…ƒã® newspaper ãƒ•ã‚©ãƒ«ãƒ€å†…ã«ä¿å­˜ã•ã‚Œã¾ã™ã€‚\n",
        "timestamp = time.strftime('%Y%m%d_%H%M%S')\n",
        "output_file = os.path.join(folder_path, f\"summary_results_{timestamp}.csv\")\n",
        "\n",
        "# ãƒ¡ã‚¤ãƒ³é–¢æ•°ã‚’å®Ÿè¡Œ\n",
        "summarize_folder_to_csv(folder_path, target_chars=target, output_csv=output_file)"
      ]
    }
  ]
}