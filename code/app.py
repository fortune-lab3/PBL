# ============================================
# Streamlit Ã— Hugging Face æ–°èåºƒå‘Šæ–‡ç”Ÿæˆã‚¢ãƒ—ãƒª
# ï¼ˆç²¾åº¦å„ªå…ˆãƒ»å¾ŒåŠå†èª¿æ•´æ–¹å¼ï¼‰
# ============================================
import os, time, re, base64
import streamlit as st
from huggingface_hub import InferenceClient
from httpx import ConnectTimeout, ReadTimeout, HTTPError

# ------------------------------------------------
# Hugging Face è¨­å®š
# ------------------------------------------------
HF_TOKEN = os.getenv("HUGGINGFACEHUB_API_TOKEN", "")
MODEL_ID = "Qwen/Qwen3-4B-Instruct-2507"

# =========================
# å‰å‡¦ç†
# =========================
def remove_strings(text: str) -> str:
    pattern = re.compile(r'ã€.*?ã€‘|[ï¼²R][ãƒ¼-]\d+|\n|\t|\s+|â– |ï¼Š')
    return pattern.sub('', text or "")

def normalize_output(text: str) -> str:
    text = (text or "").strip()
    text = re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL)
    text = re.sub(r"[a-zA-Z]+", "", text)
    return text.replace("\n", "").replace("\r", "").strip()

def count_chars(text: str) -> int:
    return len((text or "").replace("\n", "").replace("\r", ""))

# =========================
# æ–‡åˆ†å‰²
# =========================
def split_sentences(text: str) -> list[str]:
    text = normalize_output(text)
    parts = re.split(r"(ã€‚)", text)
    sentences = []
    for i in range(0, len(parts) - 1, 2):
        sentences.append(parts[i] + "ã€‚")
    return sentences

# =========================
# HF å¿œç­”æŠ½å‡º & å‘¼ã³å‡ºã—
# =========================
def _extract_message_text(choice) -> str:
    msg = getattr(choice, "message", None)
    if isinstance(msg, dict):
        return msg.get("content", "")
    return getattr(msg, "content", "")

def _call_chat(client, messages, max_tokens, temperature):
    for attempt in range(3):
        try:
            resp = client.chat.completions.create(
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
            )
            return _extract_message_text(resp.choices[0]).strip()

        except (ConnectTimeout, ReadTimeout):
            time.sleep(2 ** attempt)
        except HTTPError as e:
            if getattr(e.response, "status_code", 500) >= 500:
                time.sleep(2 ** attempt)
            else:
                raise
    return ""

# =========================
# å¾ŒåŠã ã‘å†èª¿æ•´ï¼ˆåˆ‡ã‚‰ãªã„ï¼‰
# =========================
def adjust_tail_with_llm(
    client,
    head_sentences: list[str],
    tail_sentence: str,
    target_chars: int,
    temperature: float,
) -> str:
    head = "".join(head_sentences)

    prompt = (
        f"æ¬¡ã®æ–°èåºƒå‘Šæ–‡ã®ã€å¾ŒåŠã€‘ã ã‘ã‚’ã€æ„å‘³ã‚’å¤‰ãˆãšã«è‡ªç„¶ã«æ›¸ãç›´ã—ã¦ãã ã•ã„ã€‚\n"
        f"ã€æ¡ä»¶ã€‘\n"
        f"ãƒ»å‰åŠã¯å¤‰æ›´ã—ãªã„\n"
        f"ãƒ»å…¨ä½“ãŒã ã„ãŸã„ {target_chars} æ–‡å­—å‰å¾Œã«ãªã‚‹ã‚ˆã†èª¿æ•´\n"
        f"ãƒ»æ”¹è¡Œãªã—ä¸€æ®µè½\n"
        f"ãƒ»å›ºæœ‰åè©ã‚’ä½¿ã‚ãªã„\n"
        f"ãƒ»èª‡å¤§è¡¨ç¾ã‚’é¿ã‘ã‚‹\n"
        f"ãƒ»æ–‡æœ«ã¯å¿…ãšã€Œã€‚ã€ã§çµ‚ãˆã‚‹\n\n"
        f"ã€å‰åŠã€‘\n{head}\n\n"
        f"ã€å¾ŒåŠï¼ˆä¿®æ­£å¯¾è±¡ï¼‰ã€‘\n{tail_sentence}\n\n"
        f"ã€ä¿®æ­£å¾Œã®å¾ŒåŠã€‘"
    )

    new_tail = _call_chat(
        client,
        [{"role": "user", "content": prompt}],
        max_tokens=200,
        temperature=temperature,
    )

    return head + normalize_output(new_tail)

# =========================
# åºƒå‘Šæ–‡ç”Ÿæˆï¼ˆç²¾åº¦å„ªå…ˆï¼‰
# =========================
def generate_newspaper_ad_api(text: str, target_chars: int, temperature: float = 0.2) -> str:
    if not HF_TOKEN:
        raise RuntimeError("HUGGINGFACEHUB_API_TOKEN ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

    client = InferenceClient(model=MODEL_ID, token=HF_TOKEN, timeout=60.0)

    cleaned = remove_strings(text)
    max_tokens = int(target_chars * 3)

    # â‘  ã¾ãšè‡ªç„¶ã•æœ€å„ªå…ˆã§ç”Ÿæˆ
    ad = _call_chat(
        client,
        [{"role": "user", "content": (
            f"æ¬¡ã®åŸç¨¿å†…å®¹ã‚’ã‚‚ã¨ã«ã€æ–°èã«æ²è¼‰ã§ãã‚‹åºƒå‘Šæ–‡ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\n"
            f"ãƒ»æ—¥æœ¬èªã®ã¿\n"
            f"ãƒ»å›ºæœ‰åè©ã‚’ä½¿ã‚ãªã„\n"
            f"ãƒ»æ”¹è¡Œãªã—ä¸€æ®µè½\n"
            f"ãƒ»æ–‡æœ«ã¯å¿…ãšã€Œã€‚ã€\n"
            f"ãƒ»æ–‡å­—æ•°ã¯ãŠã‚ˆã {target_chars} æ–‡å­—å‰å¾Œ\n"
            f"ãƒ»ç„¡ç†ãªæ–‡å­—æ•°åˆã‚ã›ã¯ã—ãªã„\n\n"
            f"ã€åŸç¨¿ã€‘\n{cleaned}\n\nã€åºƒå‘Šæ–‡ã€‘"
        )}],
        max_tokens=max_tokens,
        temperature=temperature,
    )

    ad = normalize_output(ad)
    sentences = split_sentences(ad)

    # â‘¡ é•·ã™ãã‚‹å ´åˆã®ã¿ã€Œæœ€å¾Œã®1æ–‡ã€ã‚’å†èª¿æ•´
    if len(sentences) >= 2 and len(ad) > target_chars + 10:
        ad = adjust_tail_with_llm(
            client,
            head_sentences=sentences[:-1],
            tail_sentence=sentences[-1],
            target_chars=target_chars,
            temperature=temperature,
        )

    return ad

# =========================
# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯
# =========================
def create_download_link(content: str, filename: str):
    b64 = base64.b64encode(content.encode()).decode()
    return f'<a href="data:file/txt;base64,{b64}" download="{filename}">ğŸ“¥ åºƒå‘Šæ–‡ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰</a>'

# =========================
# Streamlit UI
# =========================
def main():
    st.title("è¦ç´„")

    text = st.text_area("åºƒå‘Šæ–‡ã«ã—ãŸã„åŸç¨¿ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", height=260)

    target_chars = st.number_input(
        "æ–‡å­—æ•°",
        min_value=30,
        max_value=500,
        value=200,
        step=1
    )

    if st.button("åºƒå‘Šæ–‡ã‚’ç”Ÿæˆ"):
        if not text.strip():
            st.warning("åŸç¨¿ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            return

        with st.spinner("åºƒå‘Šæ–‡ã‚’ç”Ÿæˆä¸­..."):
            try:
                ad = generate_newspaper_ad_api(text, target_chars)

                st.text_area("ç”Ÿæˆã•ã‚ŒãŸåºƒå‘Šæ–‡", ad, height=200)
                st.markdown(f"æ–‡å­—æ•°ï¼š{len(ad)} æ–‡å­—")
                st.markdown(create_download_link(ad, "newspaper_ad.txt"), unsafe_allow_html=True)

            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

# =========================
# å®Ÿè¡Œ
# =========================
if __name__ == "__main__":
    main()
